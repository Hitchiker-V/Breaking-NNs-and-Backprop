{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Andrej Karpathy NN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxzBeEuf4KBMzMbEJEftIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hitchiker-V/Breaking-NNs-and-Backprop/blob/Part-1/Andrej_Karpathy_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "lehEbN3PKME0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def f(x):\n",
        "#   return 3*x**2 - 4*x + 5"
      ],
      "metadata": {
        "id": "p4GvTE2RAz_R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f(3.0)"
      ],
      "metadata": {
        "id": "ZdSvGNfIA2dp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xs = np.arange(-5,5,0.25)\n",
        "# ys = f(xs)\n",
        "# plt.plot(xs, ys)"
      ],
      "metadata": {
        "id": "wIG_IZnPA8hi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s9Q-UCcDDVX1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing operation through GraphWiz\n",
        "\n",
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "  # building unique nodes and edges\n",
        "  nodes, edges = set(), set()\n",
        "  def build(v): # v denotes Value objects\n",
        "    if v not in nodes:\n",
        "      nodes.add(v)\n",
        "      for child in v._prev:\n",
        "        edges.add((child, v))\n",
        "        build(child)\n",
        "  build(root)\n",
        "  return nodes, edges\n",
        "\n",
        "def node_id(n):\n",
        "  return str(id(n))\n",
        "\n",
        "def draw_dot(root):\n",
        "  dot = Digraph(format = 'svg', graph_attr={'rankdir': 'LR'})\n",
        "\n",
        "  nodes, edges = trace(root)\n",
        "  for n in nodes:\n",
        "    uid = str(id(n))\n",
        "    dot.node(name = uid, label = \"{%s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape = 'record')\n",
        "    if n._op:\n",
        "      # if node a result of some op, make an op node\n",
        "      dot.node(name = uid + n._op, label = n._op)\n",
        "      # connect op node to the prev node that was operated on\n",
        "      dot.edge(uid + n._op, uid)\n",
        "    \n",
        "  for n1, n2 in edges:\n",
        "      # connect n1 to op node of n2\n",
        "      dot.edge(node_id(n1), node_id(n2) + n2._op)\n",
        "\n",
        "  return dot"
      ],
      "metadata": {
        "id": "OyZaDo32B0M3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#building micrograd's Value object and internal operators\n",
        "\n",
        "# class Value:\n",
        "#   def __init__(self, data):\n",
        "#     self.data = data\n",
        "  \n",
        "#   def __repr__(self):\n",
        "#     return f\"Value(data={self.data})\"\n",
        "  \n",
        "#   def __add__(self, other):\n",
        "#     return Value(self.data + other.data)\n",
        "  \n",
        "#   def __mul__(self, other):\n",
        "#     return Value(self.data*other.data)\n",
        "\n",
        "\n",
        "# a = Value(3.0)\n",
        "# b = Value(-7.5)\n",
        "# c = Value(10.0)\n",
        "\n",
        "# d = a*b + c\n",
        "\n",
        "# To implement above function, we need any add operator to include previous operation results, hence making changes to above. Also keeping track of which variable resulted from which final operator\n",
        "# class Value:\n",
        "#   def __init__(self, data, _children = (), _op='', label =''):\n",
        "#     self.data = data\n",
        "#     self._prev = set(_children) # using set to optmize for efficiency\n",
        "#     self._op = _op \n",
        "#     self.label = label\n",
        "#   def __repr__(self):\n",
        "#     return f\"Value(data={self.data})\"\n",
        "  \n",
        "#   def __add__(self, other):\n",
        "#     return Value(self.data + other.data, (self, other), '+')\n",
        "  \n",
        "#   def __mul__(self, other):\n",
        "#     return Value(self.data*other.data, (self, other), '*')\n",
        "  \n",
        "# a = Value(3.0, label = 'a')\n",
        "# b = Value(-7.5, label = 'b')\n",
        "# c = Value(10.0, label = 'c')\n",
        "# e = a*b; e.label = 'e'\n",
        "# d = e + c; d.label = 'd'\n",
        "# f = Value(-2.0, label = 'f')\n",
        "# L = d*f; L.label = 'L'"
      ],
      "metadata": {
        "id": "OqBmaKTpBIX7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kvpykeMKYNBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing gradient\n",
        "# class Value:\n",
        "#   def __init__(self, data, _children = (), _op='', label =''):\n",
        "#     self.data = data\n",
        "#     self.grad = 0 # At initial there is no impact on the variables when data changed\n",
        "#     self._prev = set(_children) # using set to optmize for efficiency\n",
        "#     ## definining backprop step\n",
        "#     self._backward = lambda : None\n",
        "#     self._op = _op \n",
        "#     self.label = label\n",
        "#   def __repr__(self):\n",
        "#     return f\"Value(data={self.data})\"\n",
        "  \n",
        "#   def __add__(self, other):\n",
        "#     out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "#     def _backward():\n",
        "#       self.grad = 1.0*out.grad\n",
        "#       other.grad = 1.0*out.grad\n",
        "#     out._backward = _backward\n",
        "    \n",
        "#     return out\n",
        "  \n",
        "#   def __mul__(self, other):\n",
        "#     out = Value(self.data*other.data, (self, other), '*')\n",
        "\n",
        "#     def _backward():\n",
        "#       self.grad = other.data * out.grad\n",
        "#       other.grad = self.data * out.grad\n",
        "#     out._backward = _backward\n",
        "\n",
        "#     return out\n",
        "\n",
        "#   # for activation function\n",
        "#   def tanh(self):\n",
        "#     n = self.data\n",
        "#     t = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "#     out = Value(t, (self, ), 'tanh')\n",
        "\n",
        "#     def _backward():\n",
        "#       self.grad = (1-t**2) * out.grad\n",
        "#     out._backward = _backward\n",
        "\n",
        "#     return out\n",
        "\n",
        "# a = Value(3.0, label = 'a')\n",
        "# b = Value(-7.5, label = 'b')\n",
        "# c = Value(10.0, label = 'c')\n",
        "# e = a*b; e.label = 'e'\n",
        "# d = e + c; d.label = 'd'\n",
        "# f = Value(-2.0, label = 'f')\n",
        "# L = d*f; L.label = 'L'"
      ],
      "metadata": {
        "id": "WhY2qR6NRPRV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draw_dot(L)\n",
        "# grad visualizes the derivative of L wrt to L, f, d, e, c, b, a respectively"
      ],
      "metadata": {
        "id": "lNi_LpmEROt-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Implementing on a neuron\n",
        "\n",
        "# x1 = Value(2.0, label = 'x1')\n",
        "# x2 = Value(0.0, label = 'x2')\n",
        "# w1 = Value(-3.0, label = 'w1')\n",
        "# w2 = Value(1.0, label = 'w2')\n",
        "# b = Value(6.8813735870195432, label = 'b')\n",
        "\n",
        "# x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
        "# x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
        "# x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1x2w2'\n",
        "\n",
        "# n = x1w1x2w2 + b; n.label = 'n'\n",
        "# o = n.tanh(); o.label = 'o'\n",
        "# draw_dot(o)\n"
      ],
      "metadata": {
        "id": "MfEDcT0sIvUy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# o.grad = 1.0"
      ],
      "metadata": {
        "id": "QNvd13ULNG3W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# o._backward()"
      ],
      "metadata": {
        "id": "rOT6QruQZR4z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draw_dot(o)"
      ],
      "metadata": {
        "id": "RE7ehoJTZVFG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n._backward()\n",
        "# draw_dot(o)"
      ],
      "metadata": {
        "id": "GarMAGNoZZq1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x1w1x2w2._backward()\n",
        "# draw_dot(o)"
      ],
      "metadata": {
        "id": "Yi02TTokZhLH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x1w1._backward()\n",
        "# x2w2._backward()\n",
        "# draw_dot(o)"
      ],
      "metadata": {
        "id": "PLp1AkYOZm6s"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To implement backprop, the propogation should travel from top to botton : Using Topological Sort for it\n",
        "\n",
        "# o.grad = 1.0\n",
        "\n",
        "# topo = [] # Empty array containing the nodes in right order\n",
        "# visited = set() \n",
        "# def build_topo(v):\n",
        "#   if v not in visited:\n",
        "#    visited.add(v)\n",
        "#    for child in v._prev:\n",
        "#      build_topo(child)\n",
        "#    topo.append(v)\n",
        "\n",
        "# build_topo(o)\n",
        "# topo\n",
        "\n",
        "# ## Then implement _backward() method on this array sorted in topological order\n",
        "# for node in reversed(topo):\n",
        "#   node._backward()\n",
        "\n",
        "# draw_dot(o)"
      ],
      "metadata": {
        "id": "vJGfLEvYZyNv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapping inside the value class itself\n",
        "\n",
        "#Buggy Implementation\n",
        "# class Value:\n",
        "#   def __init__(self, data, _children = (), _op='', label =''):\n",
        "#     self.data = data\n",
        "#     self.grad = 0 # At initial there is no impact on the variables when data changed\n",
        "#     self._prev = set(_children) # using set to optmize for efficiency\n",
        "#     ## definining backprop step\n",
        "#     self._backward = lambda : None\n",
        "#     self._op = _op \n",
        "#     self.label = label\n",
        "#   def __repr__(self):\n",
        "#     return f\"Value(data={self.data})\"\n",
        "  \n",
        "#   def __add__(self, other):\n",
        "#     out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "#     def _backward():\n",
        "#       self.grad = 1.0*out.grad\n",
        "#       other.grad = 1.0*out.grad\n",
        "#     out._backward = _backward\n",
        "    \n",
        "#     return out\n",
        "  \n",
        "#   def __mul__(self, other):\n",
        "#     out = Value(self.data*other.data, (self, other), '*')\n",
        "\n",
        "#     def _backward():\n",
        "#       self.grad = other.data * out.grad\n",
        "#       other.grad = self.data * out.grad\n",
        "#     out._backward = _backward\n",
        "\n",
        "#     return out\n",
        "\n",
        "#   # for activation function\n",
        "#   def tanh(self):\n",
        "#     n = self.data\n",
        "#     t = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "#     out = Value(t, (self, ), 'tanh')\n",
        "\n",
        "#     def _backward():\n",
        "#       self.grad = (1-t**2) * out.grad\n",
        "#     out._backward = _backward\n",
        "\n",
        "#     return out\n",
        "  \n",
        "#   def backward(self): #topologically sorted back prop\n",
        "#     topo = [] # Empty array containing the nodes in right order\n",
        "#     visited = set() \n",
        "#     def build_topo(v):\n",
        "#       if v not in visited:\n",
        "#         visited.add(v)\n",
        "#       for child in v._prev:\n",
        "#         build_topo(child)\n",
        "#       topo.append(v)\n",
        "\n",
        "#     build_topo(self)\n",
        "\n",
        "#     self.grad = 1.0\n",
        "#     ## Then implement _backward() method on this array sorted in topological order\n",
        "#     for node in reversed(topo):\n",
        "#       node._backward()"
      ],
      "metadata": {
        "id": "dwteKL4CAUpH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Implementing on a neuron\n",
        "\n",
        "x1 = Value(2.0, label = 'x1')\n",
        "x2 = Value(0.0, label = 'x2')\n",
        "w1 = Value(-3.0, label = 'w1')\n",
        "w2 = Value(1.0, label = 'w2')\n",
        "b = Value(6.8813735870195432, label = 'b')\n",
        "\n",
        "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
        "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1x2w2'\n",
        "\n",
        "n = x1w1x2w2 + b; n.label = 'n'\n",
        "o = n.tanh(); o.label = 'o'\n",
        "draw_dot(o)\n",
        "\n",
        "o.backward()\n",
        "draw_dot(o)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "uGi9m0GUApAe",
        "outputId": "949945f4-a2ae-4de8-bc45-1fc32b9dae3e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-71e58096f06b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Implementing on a neuron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'x1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'x2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Value' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# There is a bug in the above implementation:\n",
        "# a = Value(-2.0) ; a.label = 'a'\n",
        "# b = Value(3.0) ; b.label = 'b'\n",
        "# d = a + b ; d.label = 'd'\n",
        "# e = a + b ; e.label = 'e'\n",
        "# f = d + e ; f.label = 'f'\n",
        "\n",
        "# c = a+b ; c.label = 'c'\n",
        "# d = a*b ; d.label = 'd'\n",
        "# e = c * d ; e.label = 'e'\n",
        "# draw_dot(e) \n",
        "# f.backward()\n",
        "# draw_dot(f)\n",
        "\n",
        "b = a + a ; b.label = 'b'\n",
        "b.backward()\n",
        "draw_dot(b)\n",
        "# Here a.grad should have been 2 but as in the backward(), we have self.grad = 1.0 + other.data, other.data = 1.0 + self.other\n",
        "# If self.data and other.data are same variables as in this case, the grad doesn't get updated but rather replaced.\n",
        "\n",
        "# Fix : Running summation of previous grads"
      ],
      "metadata": {
        "id": "Sv9uC17vD_dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed implementation\n",
        "# class Value:\n",
        "#   def __init__(self, data, _children = (), _op='', label =''):\n",
        "#     self.data = data\n",
        "#     self.grad = 0 # At initial there is no impact on the variables when data changed\n",
        "#     self._prev = set(_children) # using set to optmize for efficiency\n",
        "#     ## definining backprop step\n",
        "#     self._backward = lambda : None\n",
        "#     self._op = _op \n",
        "#     self.label = label\n",
        "#   def __repr__(self):\n",
        "#     return f\"Value(data={self.data})\"\n",
        "  \n",
        "#   def __add__(self, other):\n",
        "#     out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "#     def _backward():\n",
        "#       self.grad += 1.0*out.grad\n",
        "#       other.grad += 1.0*out.grad\n",
        "#     out._backward = _backward\n",
        "    \n",
        "#     return out\n",
        "  \n",
        "#   def __mul__(self, other):\n",
        "#     out = Value(self.data*other.data, (self, other), '*')\n",
        "\n",
        "#     def _backward():\n",
        "#       self.grad += other.data * out.grad\n",
        "#       other.grad += self.data * out.grad\n",
        "#     out._backward = _backward\n",
        "\n",
        "#     return out\n",
        "\n",
        "#   # for activation function\n",
        "#   def tanh(self):\n",
        "#     n = self.data\n",
        "#     t = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "#     out = Value(t, (self, ), 'tanh')\n",
        "\n",
        "#     def _backward():\n",
        "#       self.grad = (1-t**2) * out.grad\n",
        "#     out._backward = _backward\n",
        "\n",
        "#     return out\n",
        "  \n",
        "#   def backward(self): #topologically sorted back prop\n",
        "#     topo = [] # Empty array containing the nodes in right order\n",
        "#     visited = set() \n",
        "#     def build_topo(v):\n",
        "#       if v not in visited:\n",
        "#         visited.add(v)\n",
        "#       for child in v._prev:\n",
        "#         build_topo(child)\n",
        "#       topo.append(v)\n",
        "\n",
        "#     build_topo(self)\n",
        "\n",
        "#     self.grad = 1.0\n",
        "#     ## Then implement _backward() method on this array sorted in topological order\n",
        "#     for node in reversed(topo):\n",
        "#       node._backward()"
      ],
      "metadata": {
        "id": "NnzRyv72EGMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = Value(-2.0, label = 'a')\n",
        "# b = a + a ; b.label = 'b'\n",
        "# b.backward()\n",
        "# draw_dot(b)"
      ],
      "metadata": {
        "id": "4Y0BXljSIqpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x1 = Value(2.0, label = 'x1')\n",
        "# x2 = Value(0.0, label = 'x2')\n",
        "# w1 = Value(-3.0, label = 'w1')\n",
        "# w2 = Value(1.0, label = 'w2')\n",
        "# b = Value(6.8813735870195432, label = 'b')\n",
        "\n",
        "# x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
        "# x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
        "# x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1x2w2'\n",
        "\n",
        "# n = x1w1x2w2 + b; n.label = 'n'\n",
        "# o = n.tanh(); o.label = 'o'\n",
        "# draw_dot(o)\n",
        "\n",
        "# o.backward()\n",
        "# draw_dot(o)"
      ],
      "metadata": {
        "id": "2G56now_Imfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disecting tanh\n",
        "# tanh = exp(2*x) - 1 / exp(2*x) + 1\n",
        "\n",
        "# requires us to define exponential, constant multiplication, constant addition, subtraction, and division operations in Value class\n",
        "class Value:\n",
        "  def __init__(self, data, _children = (), _op='', label =''):\n",
        "    self.data = data\n",
        "    self.grad = 0 # At initial there is no impact on the variables when data changed\n",
        "    self._prev = set(_children) # using set to optmize for efficiency\n",
        "    ## definining backprop step\n",
        "    self._backward = lambda : None\n",
        "    self._op = _op \n",
        "    self.label = label\n",
        "  def __repr__(self):\n",
        "    return f\"Value(data={self.data})\"\n",
        "  \n",
        "  def __add__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += 1.0*out.grad\n",
        "      other.grad += 1.0*out.grad\n",
        "    out._backward = _backward\n",
        "    \n",
        "    return out\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data + (-1*other.data), (self, other), '-')\n",
        "\n",
        "    def _backward():\n",
        "        self.grad += -1.0*out.grad\n",
        "        other.grad += -1.0*out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "  # caking care of a + 2 and 2 + a case\n",
        "  def __radd__(self, other):\n",
        "    return self + other\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data*other.data, (self, other), '*')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "# caking care of a * 2 and 2 * a case\n",
        "\n",
        "  def __rmul__(self, other):\n",
        "    return self*other\n",
        "\n",
        "  # exp function\n",
        "  def exp(self):\n",
        "    x = self.data\n",
        "    out = Value(math.exp(x), (self, ), 'exp')\n",
        "    \n",
        "    def _backward():\n",
        "      self.grad += out.data*out.grad \n",
        "    out._backward = _backward\n",
        "    \n",
        "    return out\n",
        "\n",
        "  ##power function for a constant\n",
        "  def __pow__(self, other):\n",
        "    assert isinstance(other, (int, float)), \"support\"\n",
        "    out = Value(self.data**other, (self,), f'**{other}')\n",
        "    \n",
        "    def _backward():\n",
        "      self.grad += other*(self.data**(other-1))*out.grad \n",
        "    out._backward = _backward\n",
        "\n",
        "    return out    \n",
        "\n",
        "  # we need to define a power function as well \n",
        "  def __truediv__(self, other):\n",
        "    return self*other**-1\n",
        "\n",
        "  # for activation function\n",
        "  def tanh(self):\n",
        "    n = self.data\n",
        "    t = (math.exp(2*n)-1)/(math.exp(2*n)+1)\n",
        "    out = Value(t, (self, ), 'tanh')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = (1-t**2) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "  \n",
        "  def backward(self): #topologically sorted back prop\n",
        "    topo = [] # Empty array containing the nodes in right order\n",
        "    visited = set() \n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "      for child in v._prev:\n",
        "        build_topo(child)\n",
        "      topo.append(v)\n",
        "\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = 1.0\n",
        "    ## Then implement _backward() method on this array sorted in topological order\n",
        "    for node in reversed(topo):\n",
        "      node._backward()"
      ],
      "metadata": {
        "id": "4h7GNd4HItGo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b - a\n",
        "# Remains a bug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "5T10u4g7PTJo",
        "outputId": "828a1de4-db24-43e9-8721-fe6c218be669"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-fb6c0544d16e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Remains a bug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using pytorch for the same ops\n",
        "import torch\n",
        "x1 = torch.Tensor([2.0]).double() ; x1.requires_grad = True\n",
        "x2 = torch.Tensor([0.0]).double() ; x2.requires_grad = True\n",
        "w1 = torch.Tensor([-3.0]).double() ; w1.requires_grad = True\n",
        "w2 = torch.Tensor([1.0]).double() ; w2.requires_grad = True\n",
        "b = torch.Tensor([6.8813735870195432]).double() ; b.requires_grad = True\n",
        "\n",
        "n = x1*w1 + x2*w2 + b\n",
        "o = torch.tanh(n)\n",
        "\n",
        "print(o.data.item())\n",
        "o.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TCkKuu4PVKo",
        "outputId": "b9d7d1a8-61e8-4612-afbc-c4de666b7853"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7071066904050358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing NNs basic\n",
        "import random\n",
        "class Neuron:\n",
        "\n",
        "  # nin is number of inputs to the neurons\n",
        "  def __init__(self, nin):\n",
        "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "    self.b = Value(random.uniform(-1,1))\n",
        "  \n",
        "  def __call__(self, x): # Similar to a constructor to call the calls and underlying methods\n",
        "\n",
        "    ## zip transforms the inputs to axb dimension array with each row as a tuple of wi and xi for efficient operations\n",
        "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
        "    out = act.tanh()\n",
        "    return out\n",
        "\n",
        "\n",
        "x = [2.0, 3.0]\n",
        "n = Neuron(2)\n",
        "n(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TSf1VyDY0dK",
        "outputId": "1b992f63-7f3a-499e-a969-af2d2417f849"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Value(data=-0.6011338133373041)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing NNs as MLP as 1 Layer\n",
        "import random\n",
        "class Neuron:\n",
        "\n",
        "  # nin is number of inputs to the neurons\n",
        "  def __init__(self, nin):\n",
        "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "    self.b = Value(random.uniform(-1,1))\n",
        "  \n",
        "  def __call__(self, x): # Similar to a constructor to call the calls and \n",
        "                         # underlying methods\n",
        "\n",
        "    ## zip transforms the inputs to axb dimension array with each row as a tuple \n",
        "    ## of wi and xi for efficient operations\n",
        "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
        "    out = act.tanh()\n",
        "    return out\n",
        "\n",
        "# layer made up of individual neurons, each having nin inputs and nout outputs\n",
        "# A layer has neurons as total nuerons, hence to build a complete connected  \n",
        "#   layer, we iterate over each neurons, return the output of that neuron and  \n",
        "#   store in \"outs\" array, and return outs to the next layer as input\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self, nin, nout):\n",
        "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
        "\n",
        "  def __call__(self, x):\n",
        "    outs = [n(x) for n in self.neurons]\n",
        "    return outs\n",
        "\n",
        "\n",
        "x = [2.0, 3.0]\n",
        "# n = Neuron(2)\n",
        "n = Layer(2, 3)\n",
        "n(x)\n",
        "\n",
        "# Outputs are 3 outputs, from the 3 neurons in the final layer"
      ],
      "metadata": {
        "id": "iN_C9Jw2cU8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c6fc555-0ba2-493b-cda1-1eb3c711bef0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Value(data=-0.10455989764133997),\n",
              " Value(data=-0.5223653684280234),\n",
              " Value(data=0.9999160723656918)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing NNs as MLP as n Layer with nlay neurons list per layer\n",
        "import random\n",
        "class Neuron:\n",
        "\n",
        "  # nin is number of inputs to the neurons\n",
        "  def __init__(self, nin):\n",
        "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "    self.b = Value(random.uniform(-1,1))\n",
        "  \n",
        "  def __call__(self, x): # Similar to a constructor to call the calls and \n",
        "                         # underlying methods\n",
        "\n",
        "    ## zip transforms the inputs to axb dimension array with each row as a tuple \n",
        "    ## of wi and xi for efficient operations\n",
        "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
        "    out = act.tanh()\n",
        "    return out\n",
        "\n",
        "# layer made up of individual neurons, each having nin inputs and nout outputs\n",
        "# A layer has neurons as total nuerons, hence to build a complete connected  \n",
        "#   layer, we iterate over each neurons, return the output of that neuron and  \n",
        "#   store in \"outs\" array, and return outs to the next layer as input\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self, nin, nout):\n",
        "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
        "\n",
        "  def __call__(self, x):\n",
        "    outs = [n(x) for n in self.neurons]\n",
        "    return outs[0] if len(outs) == 1 else outs\n",
        "\n",
        "class MLP:\n",
        "  def __init__(self, nin, nouts): # Taking nin as number of inputs to MLP and nouts as a list of number of neurons per layer(including final layer) for MLP\n",
        "    size_MPL = [nin] + nouts #Converting nin and nouts to the list and adding as first layer also included in MLP\n",
        "    self.layers = [Layer(size_MPL[i], size_MPL[i+1]) for i in range(len(nouts))] # Feeding output of one layer to the next consecutive layer\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "x = [2.0, 3.0, -1]\n",
        "n = MLP(3, [4, 4, 1])\n",
        "\n",
        "n(x)\n",
        "\n",
        "# Outputs are 1 output, from the 1 neuron in the final layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HWy8vWvw1Yh",
        "outputId": "eae4eeb0-9066-4fc4-978d-7a87606fe773"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Value(data=-0.7270090561600445)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1yj-1Tryw--n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_dot(n(x))"
      ],
      "metadata": {
        "id": "TS4KEF1XzJFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's collect all parameters (w and biases) of every for, back prop to operate better\n",
        "# Implementing NNs as MLP as n Layer with nlay neurons list per layer\n",
        "\n",
        "import random\n",
        "class Neuron:\n",
        "\n",
        "  # nin is number of inputs to the neurons\n",
        "  def __init__(self, nin):\n",
        "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "    self.b = Value(random.uniform(-1,1))\n",
        "  \n",
        "  def __call__(self, x): # Similar to a constructor to call the calls and \n",
        "                         # underlying methods\n",
        "\n",
        "    ## zip transforms the inputs to axb dimension array with each row as a tuple \n",
        "    ## of wi and xi for efficient operations\n",
        "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
        "    out = act.tanh()\n",
        "    return out\n",
        "\n",
        "  def parameters(self):\n",
        "    return self.w + [self.b]\n",
        "\n",
        "# layer made up of individual neurons, each having nin inputs and nout outputs\n",
        "# A layer has neurons as total nuerons, hence to build a complete connected  \n",
        "#   layer, we iterate over each neurons, return the output of that neuron and  \n",
        "#   store in \"outs\" array, and return outs to the next layer as input\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self, nin, nout):\n",
        "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
        "\n",
        "  def __call__(self, x):\n",
        "    outs = [n(x) for n in self.neurons]\n",
        "    return outs[0] if len(outs) == 1 else outs\n",
        "\n",
        "  def params(self):\n",
        "    return [p for neurons in self.neurons for p in neurons.parameters()]\n",
        "\n",
        "class MLP:\n",
        "  def __init__(self, nin, nouts): # Taking nin as number of inputs to MLP and nouts as a list of number of neurons per layer(including final layer) for MLP\n",
        "    size_MPL = [nin] + nouts #Converting nin and nouts to the list and adding as first layer also included in MLP\n",
        "    self.layers = [Layer(size_MPL[i], size_MPL[i+1]) for i in range(len(nouts))] # Feeding output of one layer to the next consecutive layer\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "  def params_MLP(self):\n",
        "    return [p for layers in self.layers for p in layers.params()]\n",
        "\n",
        "x = [2.0, 3.0, -1]\n",
        "n = MLP(3, [4, 4, 1])\n",
        "n(x)\n",
        "\n",
        "\n",
        "# Outputs are 1 output, from the 1 neuron in the final layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOa-q9sOuVMb",
        "outputId": "a8b69ade-d436-4e81-9b93-bbc752171810"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Value(data=0.09219236495994562)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a dummy dataset\n",
        "xs = [\n",
        "    [2.0, 3.0, -1.0],\n",
        "    [3.0, -1.0, 0.5],\n",
        "    [0.5, 1.0, 1.0],\n",
        "    [1.0, 1.0, -1.0],\n",
        "]\n",
        "\n",
        "ys = [1.0, -1.0, -1.0, 1.0]\n",
        "ypred = [n(x) for x in xs]\n",
        "\n",
        "loss = sum((yout-ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c8jxd3nzcF_",
        "outputId": "7fff5e75-192c-4c61-e078-889f92d8db5e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Value(data=6.742590684127812)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Gradient descent on every parameter in the list of parameters of MLP\n",
        "alpha = 0.01\n",
        "for p in n.params_MLP():\n",
        "  p.data += alpha*p.grad"
      ],
      "metadata": {
        "id": "V8NfrqAwyqcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}